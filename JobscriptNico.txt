#!/bin/csh

# simple template job script for submission of an MPI job with
# SLURM directives

# In this script the minimum requirements have been set for SLURM
# except for these two changes that have to be made
#
# 1. replacing <number_of_tasks> for the -n option by the actual
#    number of MPI tasks required
# 2. replacing my_mpi_program by the actual executable name

# The job can be submitted with the command

# sbatch -p parallel_queue  name_of_job_script

# or with overriding the number of tasks as option

# sbatch -p parallel_queue -n number_of_tasks  name_of_job_script

# If successful SLURM will return a jobID for this job which can be
# used to query its status.

#############################################################################

## All lines that start with #SBATCH will be processed by SLURM.
## Lines in this template script that have white space between # and SBATCH
## will be ignored. They provide examples of further options that can be
## activated by deleting the white space and replacing any text after the
## option.

## By default SLURM uses as working directory the directory from where the
## job script is submitted. To change this the standard Linux cd command has
## to be used.

## Name of the job as it will appear when querying jobs with squeue (the
## default is the name of the job script)

#SBATCH  -J  ArcCont
ssh dxsb43@hamilton
## By default SLURM combines the standard output and error streams in a single
## file based on the jobID and with extension .out
## These streams can be directed to separate files with these two directives

# SBATCH  -o  out_file_name.o%j
# SBATCH  -e  err_file_name.e%j

## where SLURM will expand %j to the jobID of the job.

## Request email to the user when certain type of events occur to
## the job

#SBATCH  --mail-type=FAIL

## where <type> can be one of BEGIN, END, FAIL, REQUEUE or ALL,
## and send to email address

#SBATCH  --mail-user  nicholas.j.schliffke@durham.ac.uk
##SBATCH -t 02:00:00
#SBATCH --exclude cn7051
## The default email name is that of the submitting user as known to the system.

## Specify project or account name (currently not required).
##SBATCH -A ITSall

#############################################################################

## This job requests number_of_tasks MPI tasks (without OpenMP)

#SBATCH  -n  96

# Request submission to a queue (partition) for parallel jobs


#SBATCH  -p  par6.q

module purge
module load slurm/current
## Load any modules required here

module load openmpi/gcc/2.1.1

## Execute the MPI program

mpirun ~/bin/citcom_CircCont_nattear_par6.mpi MyinputFile.input > out
