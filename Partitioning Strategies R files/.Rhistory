paste("charset partC =", paste(partC, collapse=' '), ";"),
paste("charset partD =", paste(partD, collapse=' '), ";"),
"",
"partition chartype=4: partA, partB, partC, partD;",
"set partition=chartype;",
"",
mrBayesTemplate[(insertLine + 1L):length(mrBayesTemplate)])
outputFile <- paste0(bayesFilesDir, '/', ourFile, '.', i, '.nex')
writeLines(mrBayesOutput, outputFile)
}
}
# Mr Bayes template
mrBayesTemplate <- readLines(mrBayesTemplateFile)
insertLine <- grep(insertionComment, mrBayesTemplate)
if (!dir.exists(bayesFilesDir)) dir.create(bayesFilesDir)
powerOf2 <- 2^(0:ncol(attr(dataset, "contrast"))) #contrast shows the possible permutations of
#the character states, i.e. 0, 1, 2, 3, 4, 5, {01}, {02} etc.
decode <- apply(attr(dataset, "contrast"), 1, function(r) #
sum(powerOf2[as.logical(r)])
)
tab <- t(vapply(dataset, I, dataset[[1]])) # translates lists of taxa and character data into matrix
tab <- tab[, attr(dataset, 'index')]
nChar <- ncol(tab)
chars <- seq_len(nChar)
minSteps <- apply(tab, 2, function(char)
TreeSearch:::MinimumSteps(decode[char])
)
for (ourFile in list.files('StartingTrees', pattern='*.nex')) {
trees <- read.nexus(paste0('StartingTrees/', ourFile))
for (i in seq_along(trees)) {
tree <- trees[[i]]
parsScore <- Fitch(tree, dataset)
#calculate number to go in exp() for branch lengths prior
expVal <- ncol(tab)/parsScore
obsSteps <- FitchSteps(tree, dataset)
obsSteps <- obsSteps[attr(dataset, 'index')] #if two characters have the same profile, they are now not collapsed into one
#calculate Goloboff's unbiased measure of homoplasy for a given k (concavity constant) and data set
k <- 3
f <- (k+1)/(obsSteps+k+1+minSteps)
#rank characters by homoplasy values and then divide equally into a number of partitions
mat <- rbind(chars, f) #combine char no. and homoplasy value into one matrix
sortedMat <- mat[, order(f)] #sort columns by homoplasy (f) in ascending order
chunk <- round(prop * nChar) # number of characters per partition
partA <- sortedMat[1, 1:chunk]
partB <- sortedMat[1, (chunk+1) : (2*chunk)]
partC <- sortedMat[1, (2*chunk+1):(3*chunk)]
partD <- sortedMat[1, (3*chunk +1) : nChar]
mrBayesOutput <- c(mrBayesTemplate[seq_len(insertLine - 1)],
paste("prset brlenspr = unconstrained: exp(",expVal, ");"),
paste("charset partA =", paste(partA, collapse=' '), ";"),
paste("charset partB =", paste(partB, collapse=' '), ";"),
paste("charset partC =", paste(partC, collapse=' '), ";"),
paste("charset partD =", paste(partD, collapse=' '), ";"),
"",
"partition chartype=4: partA, partB, partC, partD;",
"set partition=chartype;",
"",
mrBayesTemplate[(insertLine + 1L):length(mrBayesTemplate)])
outputFile <- paste0(bayesFilesDir, '/', ourFile, '.', i, '.nex')
writeLines(mrBayesOutput, outputFile)
}
}
datasetName <- "OZL"
rootDir <- paste0("C:/local/dxsb43/GitHub/Partitioning_Strategies/mutations/", datasetName)
setwd(rootDir)
shellTemplateFile <- '../shell_TEMPLATE.sh.txt'
shellTemplate <- readLines(shellTemplateFile)
shellDir <- "ShellScripts"
if (!dir.exists(shellDir)) dir.create(shellDir)
m <- "TBR_chain"    ##set tree generation method
chunk1 <- c(1:20)
chunk2 <- c(21:40)
chunk3 <- c(41:60)
chunk4 <- c(61:80)
chunk5 <- c(81:100)
chunks <- list(chunk1, chunk2, chunk3, chunk4, chunk5)
for (howManyTrees in chunks) {
lines <- character(length(howManyTrees))
for (k in 1:length(howManyTrees)) {
lines[k] <- paste0('mpirun -n $SLURM_NTASKS mb /ddn/data/dxsb43/mutate',datasetName,
'/',datasetName,'_',m,'/',datasetName,'_',m,'.nex.',howManyTrees[k],'.nex')
}
shellOutput <- c(shellTemplate,
lines,
""
)
shellOutputFile <- paste0(shellDir, '/', datasetName, '_',m,'_', min(howManyTrees), '_', max(howManyTrees),
'.sh')  ##name of .sh file
#cat(shellOutput, file=shellOutputFile, sep="\n")
writeLines(shellOutput, shellOutputFile, sep="\n")
}
m <- "random"    ##set tree generation method
for (howManyTrees in chunks) {
lines <- character(length(howManyTrees))
for (k in 1:length(howManyTrees)) {
lines[k] <- paste0('mpirun -n $SLURM_NTASKS mb /ddn/data/dxsb43/mutate',datasetName,
'/',datasetName,'_',m,'/',datasetName,'_',m,'.nex.',howManyTrees[k],'.nex')
}
shellOutput <- c(shellTemplate,
lines,
""
)
shellOutputFile <- paste0(shellDir, '/', datasetName, '_',m,'_', min(howManyTrees), '_', max(howManyTrees),
'.sh')  ##name of .sh file
#cat(shellOutput, file=shellOutputFile, sep="\n")
writeLines(shellOutput, shellOutputFile, sep="\n")
}
m <- "NNI_chain"    ##set tree generation method
for (howManyTrees in chunks) {
lines <- character(length(howManyTrees))
for (k in 1:length(howManyTrees)) {
lines[k] <- paste0('mpirun -n $SLURM_NTASKS mb /ddn/data/dxsb43/mutate',datasetName,
'/',datasetName,'_',m,'/',datasetName,'_',m,'.nex.',howManyTrees[k],'.nex')
}
shellOutput <- c(shellTemplate,
lines,
""
)
shellOutputFile <- paste0(shellDir, '/', datasetName, '_',m,'_', min(howManyTrees), '_', max(howManyTrees),
'.sh')  ##name of .sh file
#cat(shellOutput, file=shellOutputFile, sep="\n")
writeLines(shellOutput, shellOutputFile, sep="\n")
}
#calculate homoplasy indices from starting trees
require(TreeSearch)
require(phangorn)
require(ape)
require(TreeSearch)
require(ape)
require(phangorn)
# Define constants
nPart <- 4 #number of partitions
prop <- 1/nPart # proportion of characters per partition
insertionComment <- "INSERT PARTITIONS HERE"
bayesFilesDir <- 'MrBayes'
# Select dataset
datasetName <- "OZL"
rootDir <- paste0("C:/local/dxsb43/GitHub/Partitioning_Strategies/mutations/", datasetName)
setwd(rootDir)
mrBayesTemplateFile <- paste0(rootDir, '/', datasetName, '_TEMPLATE.nex')
dataset <- ReadAsPhyDat(mrBayesTemplateFile)
# Mr Bayes template
mrBayesTemplate <- readLines(mrBayesTemplateFile)
insertLine <- grep(insertionComment, mrBayesTemplate)
if (!dir.exists(bayesFilesDir)) dir.create(bayesFilesDir)
powerOf2 <- 2^(0:ncol(attr(dataset, "contrast"))) #contrast shows the possible permutations of
#the character states, i.e. 0, 1, 2, 3, 4, 5, {01}, {02} etc.
decode <- apply(attr(dataset, "contrast"), 1, function(r) #
sum(powerOf2[as.logical(r)])
)
tab <- t(vapply(dataset, I, dataset[[1]])) # translates lists of taxa and character data into matrix
tab <- tab[, attr(dataset, 'index')]
nChar <- ncol(tab)
chars <- seq_len(nChar)
minSteps <- apply(tab, 2, function(char)
TreeSearch:::MinimumSteps(decode[char])
)
#make MrBayes files from perturbed trees
for (ourFile in list.files('StartingTrees', pattern='*.nex')) {
trees <- read.nexus(paste0('StartingTrees/', ourFile))
for (i in seq_along(trees)) {
tree <- trees[[i]]
parsScore <- Fitch(tree, dataset)
#calculate number to go in exp() for branch lengths prior
expVal <- ncol(tab)/parsScore
obsSteps <- FitchSteps(tree, dataset)
obsSteps <- obsSteps[attr(dataset, 'index')] #if two characters have the same profile, they are now not collapsed into one
#calculate Goloboff's unbiased measure of homoplasy for a given k (concavity constant) and data set
k <- 3
f <- (k+1)/(obsSteps+k+1+minSteps)
#rank characters by homoplasy values and then divide equally into a number of partitions
mat <- rbind(chars, f) #combine char no. and homoplasy value into one matrix
sortedMat <- mat[, order(f)] #sort columns by homoplasy (f) in ascending order
chunk <- round(prop * nChar) # number of characters per partition
partA <- sortedMat[1, 1:chunk]
partB <- sortedMat[1, (chunk+1) : (2*chunk)]
partC <- sortedMat[1, (2*chunk+1):(3*chunk)]
partD <- sortedMat[1, (3*chunk +1) : nChar]
mrBayesOutput <- c(mrBayesTemplate[seq_len(insertLine - 1)],
paste("prset brlenspr = unconstrained: exp(",expVal, ");"),
paste("charset partA =", paste(partA, collapse=' '), ";"),
paste("charset partB =", paste(partB, collapse=' '), ";"),
paste("charset partC =", paste(partC, collapse=' '), ";"),
paste("charset partD =", paste(partD, collapse=' '), ";"),
"",
"partition chartype=4: partA, partB, partC, partD;",
"set partition=chartype;",
"",
mrBayesTemplate[(insertLine + 1L):length(mrBayesTemplate)])
outputFile <- paste0(bayesFilesDir, '/', ourFile, '.', i, '.nex')
writeLines(mrBayesOutput, outputFile)
}
}
#comparing trees using similarity metrics
require(ape)
require(phytools)
require(Quartet)
require(phangorn)
require(TreeSearch)
# Define constants
nPart <- 4 #number of partitions
prop <- 1/nPart # proportion of characters per partition
insertionComment <- "INSERT PARTITIONS HERE"
bayesFilesDir <- 'MrBayes'
outputDir <- 'Results'
mrBayesTemplateFile <- paste0(rootDir, '/', datasetName, '_TEMPLATE.nex')
# Select dataset
datasetName <- "SYL"
rootDir <- paste0("C:/local/dxsb43/GitHub/Partitioning_Strategies/mutations/", datasetName)
setwd(rootDir)
if (!dir.exists('Results')) dir.create('Results')
idealTree <- read.nexus(paste0("./",datasetName, "_optimal_tree.nex"))
idealTree <- read.nexus(paste0("./",datasetName, "_optimal_tree.nex"))
dataset <- ReadAsPhyDat(mrBayesTemplateFile)
#for (ourFile in list.files('StartingTrees', pattern='*.nex'))
ourFile <- "OZL_random.nex"
for (ourFile in list.files('StartingTrees', pattern='*.nex')) {
startTrees <- read.nexus(paste0("StartingTrees/", ourFile))
resultTrees <- lapply(seq_along(startTrees), function (i)         #####works only if there are 100 con.tre's
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
#resultTrees <- lapply(c(1:20), function(i)
#                      read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
####result trees must be located in dir mutations/datasetName/MrBayes
QStatuses <- vapply(
seq_along(startTrees),
c(1:20),
function(i)
QuartetStatus(startTrees[[i]], resultTrees[[i]]),
c(N=0, Q=0, s=0, d=0, r1=0, r2=0, u=0)
)
startSim <- SimilarityMetrics(t(QStatuses))
write.csv(startSim, file=paste0('Results/', ourFile, '.start-sim.csv'))
idealStatuses <- QuartetStatus(resultTrees, cf=idealTree)
write.csv(SimilarityMetrics(idealStatuses), file=paste0('Results/', ourFile, '.ideal-sim.csv'))
startToIdealStatuses <- QuartetStatus(startTrees, cf=idealTree)
write.csv(SimilarityMetrics(startToIdealStatuses), file=paste0('Results/', ourFile, '.start-to-ideal-sim.csv'))
}
for (ourFile in list.files('StartingTrees', pattern='*.nex')) {
startTrees <- read.nexus(paste0("StartingTrees/", ourFile))
resultTrees <- lapply(seq_along(startTrees), function (i)         #####works only if there are 100 con.tre's
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
#resultTrees <- lapply(c(1:20), function(i)
#                      read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
####result trees must be located in dir mutations/datasetName/MrBayes
QStatuses <- vapply(
seq_along(startTrees),
c(1:20),
function(i)
QuartetStatus(startTrees[[i]], resultTrees[[i]]),
c(N=0, Q=0, s=0, d=0, r1=0, r2=0, u=0)
)
startSim <- SimilarityMetrics(t(QStatuses))
write.csv(startSim, file=paste0('Results/', ourFile, '.start-sim.csv'))
idealStatuses <- QuartetStatus(resultTrees, cf=idealTree)
write.csv(SimilarityMetrics(idealStatuses), file=paste0('Results/', ourFile, '.ideal-sim.csv'))
startToIdealStatuses <- QuartetStatus(startTrees, cf=idealTree)
write.csv(SimilarityMetrics(startToIdealStatuses), file=paste0('Results/', ourFile, '.start-to-ideal-sim.csv'))
}
ourFile <- list.files('StartingTrees',pattern='*.nex')[1]
startTrees <- read.nexus(paste0("StartingTrees/", ourFile))
resultTrees <- lapply(seq_along(startTrees), function (i)         #####works only if there are 100 con.tre's
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
resultTrees <- lapply(seq_along(startTrees), function (i)  #####works only if there are 100 con.tre's
i <- 1
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
i <- 1
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre"))
seq_along(startTrees)
lapply(seq_along(startTrees), function (i)  #####works only if there are 100 con.tre's
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
resultTrees <- for (i in seq_along(StartTrees)) {
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
}
resultTrees <- for (i in seq_along(StartTrees)) {
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre"))
}
resultTrees <- for (i in seq_along(startTrees)) {
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre"))
}
resultTrees <- lapply(c(1:20), function (i)  #####works only if there are 100 con.tre's
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
resultTrees <- lapply(c(1:100), function (i)  #####works only if there are 100 con.tre's
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
length(seq_along(startTrees))
i <- 54
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre"))
i <- 53
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre"))
i<- 55
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre"))
paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")
readLines(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre"))
i<- 55
readLines(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre"))
resultTrees <- lapply(seq_along(startTrees), function (i)         #####works only if there are 100 con.tre's
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
ourFile <- list.files('StartingTrees',pattern='*.nex')[2]
ourFile <- list.files('StartingTrees',pattern='*.nex')[2]
startTrees <- read.nexus(paste0("StartingTrees/", ourFile))
resultTrees <- lapply(c(1:100), function (i)  #####works only if there are 100 con.tre's
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
# if not 100 trees are used, specify the range of trees in lapply(c(...))
#resultTrees <- lapply(c(1:20), function(i)
#                      read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
####result trees must be located in dir mutations/datasetName/MrBayes
QStatuses <- vapply(
seq_along(startTrees),
c(1:20),
function(i)
QuartetStatus(startTrees[[i]], resultTrees[[i]]),
c(N=0, Q=0, s=0, d=0, r1=0, r2=0, u=0)
)
# if not 100 trees are used, specify the range of trees in lapply(c(...))
#resultTrees <- lapply(c(1:20), function(i)
#                      read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
####result trees must be located in dir mutations/datasetName/MrBayes
QStatuses <- vapply(
seq_along(startTrees),
function(i)
QuartetStatus(startTrees[[i]], resultTrees[[i]]),
c(N=0, Q=0, s=0, d=0, r1=0, r2=0, u=0)
)
startSim <- SimilarityMetrics(t(QStatuses))
write.csv(startSim, file=paste0('Results/', ourFile, '.start-sim.csv'))
idealStatuses <- QuartetStatus(resultTrees, cf=idealTree)
write.csv(SimilarityMetrics(idealStatuses), file=paste0('Results/', ourFile, '.ideal-sim.csv'))
startToIdealStatuses <- QuartetStatus(startTrees, cf=idealTree)
write.csv(SimilarityMetrics(startToIdealStatuses), file=paste0('Results/', ourFile, '.start-to-ideal-sim.csv'))
ourFile <- list.files('StartingTrees',pattern='*.nex')[3]
startTrees <- read.nexus(paste0("StartingTrees/", ourFile))
resultTrees <- lapply(c(1:100), function (i)  #####works only if there are 100 con.tre's
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
# if not 100 trees are used, specify the range of trees in lapply(c(...))
#resultTrees <- lapply(c(1:20), function(i)
#                      read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
####result trees must be located in dir mutations/datasetName/MrBayes
QStatuses <- vapply(
seq_along(startTrees),
function(i)
QuartetStatus(startTrees[[i]], resultTrees[[i]]),
c(N=0, Q=0, s=0, d=0, r1=0, r2=0, u=0)
)
startSim <- SimilarityMetrics(t(QStatuses))
write.csv(startSim, file=paste0('Results/', ourFile, '.start-sim.csv'))
idealStatuses <- QuartetStatus(resultTrees, cf=idealTree)
write.csv(SimilarityMetrics(idealStatuses), file=paste0('Results/', ourFile, '.ideal-sim.csv'))
startToIdealStatuses <- QuartetStatus(startTrees, cf=idealTree)
write.csv(SimilarityMetrics(startToIdealStatuses), file=paste0('Results/', ourFile, '.start-to-ideal-sim.csv'))
#generate plots of tree similarity.
library(readxl)
#generate plots of tree similarity.
require(readxl)
install.packages("readxl")
#generate plots of tree similarity.
require(readxl)
require(ggplot2)
require(tidyr)
#set dataset and type of baseline tree (ideal or start tree)
dataSet <- "SYL"              #CEA, OZL, SCO, SYL, THER
perturbMove <- "random"       #random, NNI_chain, TBR_chain
baseTree <- "ideal"           #ideal (=optimal), start (=tree that homoplasy was calculated based on)
#read in excel file
rootDir <-"C:/local/dxsb43/GitHub/Partitioning_Strategies/mutations"
setwd(paste0(rootDir, '/', dataSet, '/', Results))
setwd(paste0(rootDir, '/', dataSet, '/', 'Results'))
file <- paste0(dataSet, '_', perturbMove, '.nex.', baseTree, '-sim.csv')  #e.g. SYL_random.nex.ideal-sim.csv
data <- read_csv(file, skip=0, header=TRUE)
data <- read.csv(file, skip=0, header=TRUE)
data
data$StartTree <- data$X
View(data)
data$X <- data$StartTree
data[1, ]
data[,1]
data <- data[,2:10]
View(data)
data <- read.csv(file, skip=0, header=TRUE)
data$StartTree <- data$X
QD <- data$StartTree
QD <- data$QuartetDivergence
view(QD)
QD
QD$StartTree <- data$X
QD$QuartetDivergence <- data$QuartetDivergence
StartTree <- data$X
QuartetDivergence <- data$QuartetDivergence
QD <- data.frame(StartTree, QuartedDivergence)
QD <- data.frame(StartTree, QuartetDivergence)
QD
#convert data from wide to long
QD$StartTree <- factor(QD$StartTree)
#generate plots of tree similarity.
require(readxl)
require(ggplot2)
require(tidyr)
require(utils)
#set dataset and type of baseline tree (ideal or start tree)
dataSet <- "SYL"              #CEA, OZL, SCO, SYL, THER
perturbMove <- "random"       #random, NNI_chain, TBR_chain
baseTree <- "ideal"           #ideal (=optimal), start (=tree that homoplasy was calculated based on)
#read in excel file
rootDir <-"C:/local/dxsb43/GitHub/Partitioning_Strategies/mutations"
setwd(paste0(rootDir, '/', dataSet, '/', 'Results'))
file <- paste0(dataSet, '_', perturbMove, '.nex.', baseTree, '-sim.csv')  #e.g. SYL_random.nex.ideal-sim.csv
data <- read.csv(file, skip=0, header=TRUE)
StartTree <- data$X
QuartetDivergence <- data$QuartetDivergence
QD <- data.frame(StartTree, QuartetDivergence)
#convert data from wide to long
QD$StartTree <- factor(QD$StartTree)
QD$QuartetDivergence <- as.numeric(QD$QuartetDivergence)
QD$QuartetDivergence <- round(QD$QuartetDivergence, 2)
ggplot(data=QD) +
geom_point(aes(x=StartTree, y=QuartetDivergence, colour = `type of tree`)) + # aes(colour=`type of tree`)
#  facet_wrap(~ Metric) +
scale_y_continuous(name='Quartet Divergence', limits = c(0,1)) +
scale_x_discrete(name = 'Distance from published tree',breaks = c(1,10,20,30,40,50,60,70,80,90,100), limits=c(1:100)) +
ggtitle(paste0(dataSet,' Similarity of Bayesian result tree to ', baseTree, 'tree')) #+
ggplot(data=QD) +
geom_point(aes(x=StartTree, y=QuartetDivergence)) + # aes(colour=`type of tree`)
#  facet_wrap(~ Metric) +
scale_y_continuous(name='Quartet Divergence', limits = c(0,1)) +
scale_x_discrete(name = 'Distance from published tree',breaks = c(1,10,20,30,40,50,60,70,80,90,100), limits=c(1:100)) +
ggtitle(paste0(dataSet,' Similarity of Bayesian result tree to ', baseTree, 'tree')) #+
paste0(dataSet,'_',perturbMove,'_QD_vs', baseTree,'.pdf')
ggplot(data=QD) +
geom_point(aes(x=StartTree, y=QuartetDivergence)) + # aes(colour=`type of tree`)
#  facet_wrap(~ Metric) +
scale_y_continuous(name='Quartet Divergence', limits = c(0,1)) +
scale_x_discrete(name = 'Distance from published tree',breaks = c(1,10,20,30,40,50,60,70,80,90,100), limits=c(1:100)) +
ggtitle(paste0(dataSet,' Similarity of Bayesian result tree to ', baseTree, ' tree')) #+
baseTree <- "published"           #ideal (=optimal), start (=tree that homoplasy was calculated based on)
# Define constants
bayesFilesDir <- 'MrBayes'
# Select dataset
datasetName <- "SYL"
rootDir <- paste0("C:/local/dxsb43/GitHub/Partitioning_Strategies/mutations/", datasetName)
setwd(rootDir)
if (!dir.exists('Results')) dir.create('Results')
idealTree <- read.nexus(paste0("./",datasetName, "_optimal_tree.nex"))
ourFile <- list.files('StartingTrees',pattern='*.nex')[3]  ##specify perturbation method (if one of the sets of results isn't complete)
startTrees <- read.nexus(paste0("StartingTrees/", ourFile))
resultTrees <- lapply(c(1:100), function (i)  #####works only if there are 100 con.tre's
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
# if not 100 trees are used, specify the range of trees in lapply(c(...))
#resultTrees <- lapply(c(1:20), function(i)
#                      read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
####result trees must be located in dir mutations/datasetName/MrBayes
QStatuses <- vapply(
seq_along(startTrees),
function(i)
QuartetStatus(startTrees[[i]], resultTrees[[i]]),
c(N=0, Q=0, s=0, d=0, r1=0, r2=0, u=0)
)
idealStatuses <- QuartetStatus(resultTrees, cf=idealTree)
write.csv(SimilarityMetrics(idealStatuses), file=paste0('Results/', ourFile, '.published-sim.csv'))
startToIdealStatuses <- QuartetStatus(startTrees, cf=idealTree)
write.csv(SimilarityMetrics(startToIdealStatuses), file=paste0('Results/', ourFile, '.start-to-published-sim.csv'))
baseTree <- "published"           #published (=optimal), start (=tree that homoplasy was calculated based on)
#read in excel file
rootDir <-"C:/local/dxsb43/GitHub/Partitioning_Strategies/mutations"
setwd(paste0(rootDir, '/', dataSet, '/', 'Results'))
file <- paste0(dataSet, '_', perturbMove, '.nex.', baseTree, '-sim.csv')  #e.g. SYL_random.nex.ideal-sim.csv
data <- read.csv(file, skip=0, header=TRUE)
StartTree <- data$X
ourFile <- list.files('StartingTrees',pattern='*.nex')[2]  ##specify perturbation method (if one of the sets of results isn't complete)
startTrees <- read.nexus(paste0("StartingTrees/", ourFile))
resultTrees <- lapply(c(1:100), function (i)  #####works only if there are 100 con.tre's
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
rootDir <- paste0("C:/local/dxsb43/GitHub/Partitioning_Strategies/mutations/", datasetName)
setwd(rootDir)
if (!dir.exists('Results')) dir.create('Results')
idealTree <- read.nexus(paste0("./",datasetName, "_optimal_tree.nex"))
ourFile <- list.files('StartingTrees',pattern='*.nex')[2]  ##specify perturbation method (if one of the sets of results isn't complete)
startTrees <- read.nexus(paste0("StartingTrees/", ourFile))
resultTrees <- lapply(c(1:100), function (i)  #####works only if there are 100 con.tre's
read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
# if not 100 trees are used, specify the range of trees in lapply(c(...))
#resultTrees <- lapply(c(1:20), function(i)
#                      read.nexus(paste0(rootDir,"/",bayesFilesDir, "/", ourFile, '.', i,".nex.con.tre")))
####result trees must be located in dir mutations/datasetName/MrBayes
QStatuses <- vapply(
seq_along(startTrees),
function(i)
QuartetStatus(startTrees[[i]], resultTrees[[i]]),
c(N=0, Q=0, s=0, d=0, r1=0, r2=0, u=0)
)
startSim <- SimilarityMetrics(t(QStatuses))
write.csv(startSim, file=paste0('Results/', ourFile, '.start-sim.csv'))
idealStatuses <- QuartetStatus(resultTrees, cf=idealTree)
write.csv(SimilarityMetrics(idealStatuses), file=paste0('Results/', ourFile, '.published-sim.csv'))
startToIdealStatuses <- QuartetStatus(startTrees, cf=idealTree)
write.csv(SimilarityMetrics(startToIdealStatuses), file=paste0('Results/', ourFile, '.start-to-published-sim.csv'))
#read in excel file
rootDir <-"C:/local/dxsb43/GitHub/Partitioning_Strategies/mutations"
setwd(paste0(rootDir, '/', dataSet, '/', 'Results'))
file <- paste0(dataSet, '_', perturbMove, '.nex.', baseTree, '-sim.csv')  #e.g. SYL_random.nex.ideal-sim.csv
data <- read.csv(file, skip=0, header=TRUE)
StartTree <- data$X
QuartetDivergence <- data$QuartetDivergence
QD <- data.frame(StartTree, QuartetDivergence)
#convert data from wide to long
QD$StartTree <- factor(QD$StartTree)
QD$QuartetDivergence <- as.numeric(QD$QuartetDivergence)
QD$QuartetDivergence <- round(QD$QuartetDivergence, 2)
ggplot(data=QD) +
geom_point(aes(x=StartTree, y=QuartetDivergence)) + # aes(colour=`type of tree`)
#  facet_wrap(~ Metric) +
scale_y_continuous(name='Quartet Divergence', limits = c(0,1)) +
scale_x_discrete(name = paste0('Distance from ', baseTree, 'tree'),breaks = c(1,10,20,30,40,50,60,70,80,90,100), limits=c(1:100)) +
ggtitle(paste0(dataSet,' Similarity of Bayesian result tree to ', baseTree, ' tree')) #+
ggsave(filename=paste0(dataSet,'_',perturbMove,'_QD_vs_', baseTree,'.pdf'), path="C:/local/dxsb43/GitHub/Partitioning_Strategies/mutations/SimPlots")
